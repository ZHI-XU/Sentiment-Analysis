{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ExtractMetrics4IMDB.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "mount_file_id": "1Ah67RqIE8A5-0894lR-PDC0fwtGMiC-7",
   "authorship_tag": "ABX9TyNgzI7o9sNW9yMwJcNNkDb8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeuSKlYC_1yr"
   },
   "source": [
    "# CoreNLP server"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bioNhcRu-qa6"
   },
   "source": [
    "# init and start CoreNLP server\n",
    "import os\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_d1tOuFkKMAv"
   },
   "source": [
    "!wget \"https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\"\n",
    "!unzip \"stanford-corenlp-latest.zip\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZK29NaSLqvr",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1618819914219,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     },
     "user_tz": -480
    },
    "outputId": "f29afe85-6d70-48cd-d1ce-e5d3de0af21e"
   },
   "source": [
    "cd stanford-corenlp-4.2.0/\n",
    "!java -mx6g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 30000\n",
    "\n",
    "# shut down CoreNLP server\n",
    "\n",
    "# !ps aux | grep java\n",
    "# !kill 719"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/stanford-corenlp-4.2.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCrb5qdmM7MP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618838686622,
     "user_tz": -480,
     "elapsed": 3689,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    },
    "outputId": "dab3dcc8-591d-4964-953f-d3df8ad9ceea"
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olEdIHI7ADDM"
   },
   "source": [
    "# Code of extracting metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBsGzFlR9nTk"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.parse.corenlp import CoreNLPServer, CoreNLPParser\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#\n",
    "# nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "# parser = CoreNLPParser(url='http://localhost:9001')"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-a82eeec5d22d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;31m# warnings.simplefilter(action='ignore', category=FutureWarning)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[0mnlp_pipeline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"en_core_web_sm\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCoreNLPParser\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'http://localhost:9001'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\spacy\\__init__.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[0menable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0menable\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m         \u001B[0mexclude\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexclude\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m         \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m     )\n\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\spacy\\util.py\u001B[0m in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m    437\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    438\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mE941\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfull\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[index]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 439\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mE050\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    440\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB3mAjVi8k9-"
   },
   "source": [
    "## Input feature metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Bar_g3v_O2Y"
   },
   "source": [
    "def lexical(sentence):\n",
    "    doc = nlp_pipeline(sentence)\n",
    "    oov = 0\n",
    "    # verb\n",
    "    VB = 0\n",
    "    # noun\n",
    "    NN = 0\n",
    "    # adj\n",
    "    JJ = 0\n",
    "    # adv\n",
    "    RB = 0\n",
    "    # conj\n",
    "    CONJ = 0\n",
    "\n",
    "    length = 0\n",
    "\n",
    "    for token in doc:\n",
    "        length += 1\n",
    "        if token.tag_.startswith(\"J\"):\n",
    "            JJ += 1\n",
    "        if token.tag_.startswith(\"N\"):\n",
    "            NN += 1\n",
    "        if token.tag_.startswith(\"R\"):\n",
    "            RB += 1\n",
    "        if token.tag_.startswith(\"V\"):\n",
    "            VB += 1\n",
    "        if token.tag_.startswith(\"C\"):\n",
    "            CONJ += 1\n",
    "\n",
    "    return VB, NN, JJ, RB, CONJ, len(list(doc.sents)), length\n",
    "\n",
    "# Degree of polysemy\n",
    "def polysemy(clean_line):\n",
    "    polysemyCount = 0\n",
    "    words = clean_line.split(\" \")\n",
    "    for w in words:\n",
    "        polysemyCount += len(wn.synsets(w))\n",
    "    return float(polysemyCount) / float(len(words))\n",
    "\n",
    "\n",
    "# dependency distance\n",
    "def depend_dist(sentence):\n",
    "    doc = nlp_pipeline(sentence)\n",
    "    sum_dist = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_dist = 0\n",
    "        for token in sent:\n",
    "            if not token.is_punct:\n",
    "                for child in token.children:\n",
    "                    sent_dist += abs(token.i - child.i)\n",
    "        sum_dist += sent_dist\n",
    "    return float(sum_dist) / float(len(list(doc.sents)))\n",
    "\n",
    "\n",
    "# height of constituency parsing tree\n",
    "def const_parse(sentences):\n",
    "    doc = nlp_pipeline(sentences)\n",
    "    sum_height = 0\n",
    "    non_term_count = 0\n",
    "    term_count = 0\n",
    "    for sent in doc.sents:\n",
    "        if sent:\n",
    "            try:\n",
    "                res = next(parser.raw_parse(sent.text))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(sent)\n",
    "            else:\n",
    "                sum_height += res.height()\n",
    "                non_term_count += len(list(res.leaves()))\n",
    "                term_count += len(list(res.subtrees()))\n",
    "\n",
    "    return (\n",
    "        float(sum_height) / float(len(list(doc.sents))),\n",
    "        float(non_term_count) / float(term_count),\n",
    "    )\n",
    "\n",
    "\n",
    "# process sentiment score\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentiment(word, tag):\n",
    "    \"\"\" \n",
    "    returns list of pos neg and objective score. But returns empty list \n",
    "    if not present in senti wordnet. \n",
    "    \"\"\"\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return [0, 1, 0]\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [\n",
    "            swn_synset.pos_score(), \n",
    "            swn_synset.neg_score(), \n",
    "            swn_synset.obj_score(),\n",
    "    ]\n",
    "\n",
    "\n",
    "def senti_pro(clean_sent):\n",
    "    flip_count = 0\n",
    "    words = clean_sent.split(\" \")\n",
    "    pos_val = nltk.pos_tag(words)\n",
    "    senti_ret = np.array([get_sentiment(x, y) for (x, y) in pos_val])\n",
    "    senti_val = senti_ret.T[0] - senti_ret.T[1]\n",
    "    senti_score = abs(senti_ret.T[0].sum() - senti_ret.T[1].sum())\n",
    "    \n",
    "    # sentiment flip count\n",
    "    for i in range(senti_val.shape[0] - 1):\n",
    "        if senti_val[i] * senti_val[i + 1] < 0:\n",
    "            flip_count += 1\n",
    "\n",
    "    return senti_score, flip_count"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE9CjLqKCqBR"
   },
   "source": [
    "## Fetch&Process data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vdDiSldbFDLC"
   },
   "source": [
    "def modi_data(obj_df):\n",
    "    obj_df = obj_df[obj_df[\"label\"] != 2]\n",
    "    # obj_df[\"label\"] = obj_df[\"label\"].astype(string)\n",
    "    obj_df.label = obj_df.label.replace(0, \"neg\")\n",
    "    obj_df.label = obj_df.label.replace(1, \"neg\")\n",
    "    obj_df.label = obj_df.label.replace(3, \"pos\")\n",
    "    obj_df.label = obj_df.label.replace(4, \"pos\")\n",
    "    obj_df = obj_df.fillna(0)\n",
    "    # obj_df[\"label\"] = obj_df[\"label\"].astype(int)\n",
    "    return obj_df\n",
    "\n",
    "\n",
    "def clean_sent(sentence):\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pre_proc(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    if len(words) > max_len:\n",
    "        sentence = \" \".join(words[:max_len])\n",
    "    return sentence"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "83IzbZBr-ND9"
   },
   "source": [
    "# set global var\n",
    "\n",
    "data_path = \"./IMDB/\"\n",
    "tokenizer_path = \"./tokenizer/200in_IMDB.pickle\"\n",
    "max_len = 200\n",
    "remove_threshold = 1e-5 "
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MEK6WSlyl0j0"
   },
   "source": [
    "train_df = pd.read_csv(\n",
    "    data_path + \"train.csv\", \n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    data_path + \"test.csv\", \n",
    ")\n",
    "\n",
    "\n",
    "with open(tokenizer_path, \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# test_df = modi_data(test_df)\n",
    "test_sentence = test_df.text.tolist()\n",
    "# test_clean_tokens = [clean_sent(i) for i in test_sentence]\n",
    "test_x = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_x = pad_sequences(test_x, maxlen=max_len, padding=\"post\")\n",
    "test_y = test_df.label.to_numpy().reshape(-1,1)\n",
    "test_sentence = list(map(pre_proc, test_sentence))\n",
    "\n",
    "test_label = test_df.label.to_numpy().astype(\"object\")\n",
    "test_label[test_label == 0] = 'neg'\n",
    "test_label[test_label == 1] = 'pos'"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YGA8aCQl6YtI"
   },
   "source": [
    "# clean_line = [\" \".join(n) for n in test_clean_tokens]\n",
    "inp_feature_df = pd.DataFrame(\n",
    "    {\"Sentence\": test_sentence, \n",
    "    #  \"CleanLine\": clean_line, \n",
    "     \"TrueRes\": test_y.reshape(-1)}\n",
    ")\n",
    "\n",
    "inp_feature_df[\"VBCount\"], inp_feature_df[\"NNCount\"], \\\n",
    "inp_feature_df[\"JJCount\"], inp_feature_df[\"RBCount\"], \\\n",
    "inp_feature_df[\"ConjCount\"], inp_feature_df[\"SentCount\"], \\\n",
    "inp_feature_df[\"Length\"] = zip(\n",
    "    *inp_feature_df[\"Sentence\"].map(lexical)\n",
    ")\n",
    "\n",
    "inp_feature_df[\"Polysemy\"] = inp_feature_df[\"Sentence\"].map(polysemy)\n",
    "\n",
    "# dependency distance\n",
    "inp_feature_df[\"DependDist\"] = inp_feature_df[\"Sentence\"].map(depend_dist)\n",
    "\n",
    "# sentiment process\n",
    "inp_feature_df[\"SentiScore\"], inp_feature_df[\"SentiFlip\"] = zip(\n",
    "    *inp_feature_df[\"Sentence\"].map(senti_pro)\n",
    ")\n",
    "\n",
    "# constituency parsing tree\n",
    "inp_feature_df[\"ConstHeight\"], inp_feature_df[\"TerminalRatio\"] = \\\n",
    "zip(*inp_feature_df[\"Sentence\"].map(const_parse))\n",
    "\n",
    "\n",
    "inp_feature_df.to_csv(\"./Metrics/IMDB_inp.csv\", index=False)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-13-30b36bfc6b0e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0minp_feature_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"ConjCount\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minp_feature_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"SentCount\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m inp_feature_df[\"Length\"] = zip(\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[1;33m*\u001B[0m\u001B[0minp_feature_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Sentence\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlexical\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mmap\u001B[1;34m(self, arg, na_action)\u001B[0m\n\u001B[0;32m   3628\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3629\u001B[0m         \"\"\"\n\u001B[1;32m-> 3630\u001B[1;33m         \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_map_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_action\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mna_action\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3631\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_constructor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnew_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__finalize__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3632\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\pandas\\core\\base.py\u001B[0m in \u001B[0;36m_map_values\u001B[1;34m(self, mapper, na_action)\u001B[0m\n\u001B[0;32m   1143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1144\u001B[0m         \u001B[1;31m# mapper is a function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1145\u001B[1;33m         \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmap_f\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmapper\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1147\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mnew_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m<ipython-input-8-ec52ffd38065>\u001B[0m in \u001B[0;36mlexical\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mlexical\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnlp_pipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[0moov\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;31m# verb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mVB\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nlp_pipeline' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EYjMh3-G0Wp"
   },
   "source": [
    "## Output feature metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fpcnoH2h8y44"
   },
   "source": [
    "def get_last_layer_model(model):\n",
    "    layer_names = [layer.name for layer in model.layers]\n",
    "    layer_output = model.get_layer(layer_names[-2]).output\n",
    "    ret = Model(model.input, layer_output)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_train_at(train_df, last_layer_model, train_at):\n",
    "    pos_df = train_df[train_df[\"label\"] == 1]\n",
    "    neg_df = train_df[train_df[\"label\"] == 0]\n",
    "\n",
    "    pos_sentence = pos_df.text.tolist()\n",
    "    pos_x = tokenizer.texts_to_sequences(pos_sentence)\n",
    "    pos_x = pad_sequences(pos_x, maxlen = max_len, padding=\"post\")\n",
    "\n",
    "    neg_sentence = neg_df.text.tolist()\n",
    "    neg_x = tokenizer.texts_to_sequences(neg_sentence)\n",
    "    neg_x = pad_sequences(neg_x, maxlen = max_len, padding=\"post\")\n",
    "    \n",
    "    train_at[\"pos\"] = last_layer_model.predict(pos_x)\n",
    "    train_at[\"neg\"] = last_layer_model.predict(neg_x)\n",
    "\n",
    "def get_kernels(train_at):\n",
    "    removed_cols={'pos': [], 'neg': []}\n",
    "\n",
    "    for i in range(train_at[\"pos\"].T.shape[0]):\n",
    "        if np.var(train_at[\"pos\"].T[i]) < remove_threshold:\n",
    "            removed_cols['pos'].append(i)\n",
    "    for i in range(train_at[\"neg\"].T.shape[0]):\n",
    "        if np.var(train_at[\"neg\"].T[i]) < remove_threshold:\n",
    "            removed_cols['neg'].append(i)\n",
    "\n",
    "    pos_vals = np.delete(train_at[\"pos\"].T, removed_cols['pos'], axis=0)\n",
    "    neg_vals = np.delete(train_at[\"neg\"].T, removed_cols['neg'], axis=0)\n",
    "\n",
    "    kernels={}\n",
    "    kernels[\"pos\"] = stats.gaussian_kde(pos_vals)\n",
    "    kernels[\"neg\"] = stats.gaussian_kde(neg_vals)\n",
    "\n",
    "    return kernels, removed_cols\n",
    "    \n",
    "\n",
    "def get_lsa(kernels, removed_cols, test_pred, test_label):\n",
    "    lsa=[]\n",
    "    \n",
    "    for i in range(len(test_pred)):\n",
    "        value = np.delete(test_pred[i], removed_cols[test_label[i]])\n",
    "        temp = np.negative(np.log(kernels[test_label[i]](value)))\n",
    "        \n",
    "        lsa.append(temp[0])\n",
    "    \n",
    "    return lsa\n",
    "\n",
    "def find_closest_at(at, train_at):\n",
    "    \"\"\"The closest distance between subject AT and training ATs.\n",
    "    Args:\n",
    "        at (list): List of activation traces of an input.        \n",
    "        train_at (list): List of activation traces in training set (filtered)\n",
    "        \n",
    "    Returns:\n",
    "        dist (int): The closest distance.\n",
    "        at (list): Training activation trace that has the closest distance.\n",
    "    \"\"\"\n",
    "\n",
    "    dist = np.linalg.norm(at - train_at, axis=1)\n",
    "    return (min(dist), train_at[np.argmin(dist)])\n",
    "\n",
    "\n",
    "def get_dsa(test_pred, test_label, train_at):\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(len(test_pred)):\n",
    "        label = test_label[i]\n",
    "        at = test_pred[i]\n",
    "        a_dist, a_dot = find_closest_at(at, train_at[label])\n",
    "        b_dist, _ = find_closest_at(\n",
    "            a_dot, train_at[list(set([\"pos\", \"neg\"]) - set([label]))[0]]\n",
    "        )\n",
    "        ret.append(a_dist / b_dist)\n",
    "    return ret\n",
    "\n",
    "def val_to_res(val):\n",
    "    if val > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def deep_gini(val):\n",
    "    ret = float(1 - pow(val, 2) - pow(1 - val, 2))\n",
    "    return ret\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wnRpXLYumtNH"
   },
   "source": [
    "def Process(model_path, out_path):\n",
    "    dirs = os.listdir(model_path)\n",
    "    for i in dirs:\n",
    "        if os.path.splitext(i)[1] == \".hdf5\":\n",
    "            # model = get_Trans(max_len)\n",
    "            # model.load_weights(model_path + i)\n",
    "            model = load_model(model_path + i)\n",
    "\n",
    "            # verify\n",
    "            # test_scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "            print(i, \"start\")\n",
    "            # if test_scores[1] < float(\"0.\" + i.split('_')[1][:4]):\n",
    "            #     raise Exception(\"Invalid acc! \" + i, test_scores[1])\n",
    "            \n",
    "            pred_val = model.predict(test_x)\n",
    "            pred_res = [val_to_res(n) for n in pred_val]\n",
    "            output_feature_df = pd.DataFrame(\n",
    "                {\"PredVal\": pred_val[:, 0], \"PredRes\": pred_res}\n",
    "            )\n",
    "            output_feature_df[\"DeepGini\"] = \\\n",
    "            output_feature_df[\"PredVal\"].map(deep_gini)\n",
    "            \n",
    "            last_layer_model = get_last_layer_model(model)\n",
    "            test_pred = last_layer_model.predict(test_x)\n",
    "            train_at = {}\n",
    "            get_train_at(train_df, last_layer_model, train_at)\n",
    "            kernels, removed_cols = get_kernels(train_at)\n",
    "            lsa = get_lsa(kernels, removed_cols, test_pred, test_label)\n",
    "            dsa = get_dsa(test_pred, test_label, train_at)\n",
    "            output_feature_df[\"LSA\"] = lsa\n",
    "            output_feature_df[\"DSA\"] = dsa\n",
    "            \n",
    "            res_df = pd.concat([inp_feature_df, output_feature_df], axis=1)\n",
    "\n",
    "            res_df[\"isRight\"] = res_df.apply(\n",
    "                lambda x: x[\"TrueRes\"] == x[\"PredRes\"], axis=1\n",
    "            )\n",
    "            res_df.to_csv(out_path + os.path.splitext(i)[0] + \".csv\")\n",
    "            print(\"model \" + os.path.splitext(i)[0] + \" is saved\")\n",
    "    print(\"done!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2jjFdV4hO7Du",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619023079565,
     "user_tz": -480,
     "elapsed": 2886160,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    },
    "outputId": "8b9f7999-1a2b-45a4-8c5b-bbc0283e2c38"
   },
   "source": [
    "model_path = \"./Model/LSTM_IMDB/\"\n",
    "out_path = \"./Metrics/LSTM_IMDB/\"\n",
    "inp_feature_df = pd.read_csv(\n",
    "        \"./Metrics/IMDB_inp.csv\",\n",
    "    )\n",
    "\n",
    "Process(model_path, out_path)"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "3469_8651.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3469_8651 is saved\n",
      "3428_8604.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3428_8604 is saved\n",
      "3439_8611.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3439_8611 is saved\n",
      "3452_8624.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3452_8624 is saved\n",
      "3497_8618.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3497_8618 is saved\n",
      "3502_8677.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3502_8677 is saved\n",
      "3368_8629.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3368_8629 is saved\n",
      "3512_8633.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3512_8633 is saved\n",
      "3336_8661.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3336_8661 is saved\n",
      "3238_8646.hdf5 start\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "model 3238_8646 is saved\n",
      "done!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUEAhmyurSyt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619006458018,
     "user_tz": -480,
     "elapsed": 2321,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    },
    "outputId": "f47066d1-e681-430d-d9d6-6d5f1b30704a"
   },
   "source": [
    " os.listdir(model_path)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['3469_8651.hdf5',\n",
       " '3428_8604.hdf5',\n",
       " '3439_8611.hdf5',\n",
       " '3452_8624.hdf5',\n",
       " '3497_8618.hdf5',\n",
       " '3502_8677.hdf5',\n",
       " '3368_8629.hdf5',\n",
       " '3512_8633.hdf5',\n",
       " '3336_8661.hdf5',\n",
       " '3238_8646.hdf5']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xZAC1MOU-jOd"
   },
   "source": [
    "model = load_model(model_path + '2748_8855.hdf5')\n",
    "pred_val = model.predict(test_x)\n",
    "pred_res = [val_to_res(n) for n in pred_val]\n",
    "output_feature_df = pd.DataFrame(\n",
    "    {\"PredVal\": pred_val[:, 0], \"PredRes\": pred_res}\n",
    ")\n",
    "output_feature_df[\"DeepGini\"] = \\\n",
    "output_feature_df[\"PredVal\"].map(deep_gini)\n",
    "            "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fbdt_7lqahYQ"
   },
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Add,\n",
    "    Lambda,\n",
    "    Dropout,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'maxlen': self.maxlen,\n",
    "            'embed_dim': self.embed_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def get_Trans(max_len):\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_len, len(tokenizer.word_index) + 1, embed_dim)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\", name=\"input\")\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    o = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=o)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
