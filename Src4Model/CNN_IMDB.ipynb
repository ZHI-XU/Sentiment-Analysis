{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"CNN_IMDB.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gwcXt_Kcne5N"},"source":["# CNN"]},{"cell_type":"code","metadata":{"id":"tq9imcf0ne5o","executionInfo":{"status":"ok","timestamp":1621006266055,"user_tz":-480,"elapsed":4724,"user":{"displayName":"柳箜铭","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64","userId":"16745569133448555194"}}},"source":["import os\n","import numpy as np\n","import string\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import pickle\n","import pandas as pd\n","\n","\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.activations import relu\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import (\n","    Input,\n","    Dense,\n","    Embedding,\n","    Flatten,\n","    Conv1D,\n","    MaxPooling1D,\n","    Add,\n","    Lambda,\n","    Dropout,\n","    concatenate,\n",")\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.backend import l2_normalize\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","from sklearn.metrics import classification_report\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from IPython.display import SVG"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_GjM47tne5r"},"source":["def ModiData(obj_df):\n","    obj_df = obj_df[obj_df[\"label\"] != 2]\n","    # obj_df[\"label\"] = obj_df[\"label\"].astype(string)\n","    obj_df.label = obj_df.label.replace(0, \"neg\")\n","    obj_df.label = obj_df.label.replace(1, \"neg\")\n","    obj_df.label = obj_df.label.replace(3, \"pos\")\n","    obj_df.label = obj_df.label.replace(4, \"pos\")\n","    obj_df = obj_df.fillna(0)\n","    # obj_df[\"label\"] = obj_df[\"label\"].astype(int)\n","    return obj_df\n","\n","\n","def clean_line(line):\n","    # split into tokens by white space\n","    tokens = line.split()\n","    # remove punctuation from each token\n","    table = str.maketrans(\"\", \"\", string.punctuation)\n","    tokens = [w.translate(table) for w in tokens]\n","    # remove remaining tokens that are not alphabetic\n","    tokens = [word for word in tokens if word.isalpha()]\n","    # filter out stop words\n","    stop_words = set(stopwords.words(\"english\"))\n","    tokens = [w for w in tokens if not w in stop_words]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) > 1]\n","    return tokens\n","\n","\n","def create_tokenizer(lines):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(lines)\n","    return tokenizer\n","\n","\n","def encode_text(tokenizer, lines, length):\n","    # integer encode\n","    encoded = tokenizer.texts_to_sequences(lines)\n","    # pad encoded sequences\n","    padded = pad_sequences(encoded, maxlen=length, padding=\"post\")\n","    return padded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"546eDp-n7Wtp","executionInfo":{"status":"ok","timestamp":1621006272074,"user_tz":-480,"elapsed":6013,"user":{"displayName":"柳箜铭","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64","userId":"16745569133448555194"}}},"source":["train_df = pd.read_csv(\n","    \"/content/drive/MyDrive/Sentiment Analysis/IMDB/train.csv\", \n",")\n","\n","test_df = pd.read_csv(\n","    \"/content/drive/MyDrive/Sentiment Analysis/IMDB/test.csv\", \n",")\n","\n","train_df = train_df.sample(frac=1).reset_index(drop=True)\n","test_df = test_df.sample(frac=1).reset_index(drop=True)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoxxs-jane5t"},"source":["# train_df = ModiData(train_df)\n","# # dev_df = ModiData(dev_df)\n","# test_df = ModiData(test_df)\n","\n","train_x = train_df.text.tolist()\n","train_y = train_df.label\n","# dev_x = dev_df.text.tolist()\n","# dev_y = dev_df.label\n","test_x = test_df.text.tolist()\n","test_y = test_df.label\n","\n","max_len = 200\n","# tokenizer = create_tokenizer(train_x)\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/tokenizer/200in_IMDB.pickle\", \"rb\") as handle:\n","    tokenizer = pickle.load(handle)\n","# calculate vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n","word_index = tokenizer.word_index\n","train_x = encode_text(tokenizer, train_x, max_len)\n","# dev_x = encode_text(tokenizer, dev_x, max_len)\n","test_x = encode_text(tokenizer, test_x, max_len)\n","\n","le = LabelEncoder()\n","train_y = le.fit_transform(train_y).reshape(-1, 1)\n","# dev_y = le.transform(dev_y).reshape(-1, 1)\n","test_y = le.transform(test_y).reshape(-1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtoxFTHhne5v"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"b8g1hAT9ne5w"},"source":["def get_embeddings_layer(name, max_len, trainable):\n","    embedding_layer = Embedding(\n","        input_dim=vocab_size,\n","        output_dim=50,\n","        input_length=max_len,\n","        # weights=[embeddings_matrix],\n","        trainable=trainable,\n","        name=name,\n","    )\n","    return embedding_layer\n","\n","\n","def get_conv_pool(x_input, max_len, sufix, n_grams=[3, 4, 5], feature_maps=100):\n","    branches = []\n","    for n in n_grams:\n","        branch = Conv1D(\n","            filters=feature_maps,\n","            kernel_size=n,\n","            activation=relu,\n","            name=\"Conv_\" + sufix + \"_\" + str(n),\n","        )(x_input)\n","        branch = MaxPooling1D(\n","            pool_size=max_len - n + 1,\n","            strides=None,\n","            padding=\"valid\",\n","            name=\"MaxPooling_\" + sufix + \"_\" + str(n),\n","        )(branch)\n","        branch = Flatten(name=\"Flatten_\" + sufix + \"_\" + str(n))(branch)\n","        branches.append(branch)\n","    return branches\n","\n","\n","def get_cnn_rand(max_len):\n","    embeddings_layer_channel_1 = get_embeddings_layer(\n","        \"embeddings_layer_dynamic\", max_len, trainable=True\n","    )\n","\n","    input_dynamic = Input(shape=(max_len,), dtype=\"int32\", name=\"input_dynamic\")\n","    x = embeddings_layer_channel_1(input_dynamic)\n","    branches_dynamic = get_conv_pool(x, max_len, \"dynamic\")\n","    z_dynamic = concatenate(branches_dynamic, axis=-1)\n","\n","    z = Dropout(0.1)(z_dynamic)\n","    z = layer = Dense(20, activation=\"relu\", name=\"FC1\")(z)\n","    # z = Dropout(0.1)(z)\n","    o = Dense(1, activation=\"sigmoid\", name=\"output\")(z)\n","\n","    model = Model(inputs=input_dynamic, outputs=o)\n","    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n19GzqOvne5z"},"source":["class myCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","        if logs.get(\"val_loss\") < 0.28:\n","            print(\"\\nReached 0.27 loss so cancelling training!\")\n","            self.model.stop_training = True\n","\n","\n","beststop = myCallback()\n","earlystop = EarlyStopping(monitor=\"val_loss\", min_delta=0.0001)\n","checkpointer = ModelCheckpoint(filepath=\"./weights.hdf5\", verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_NteEQzne50"},"source":["def TrainModel(num):\n","    while num > 0:\n","        model = get_cnn_rand(max_len)\n","        # model.summary()\n","        history = model.fit(\n","            x=train_x,\n","            y=train_y,\n","            batch_size=100,\n","            epochs=20,\n","            validation_data=(test_x, test_y),\n","            callbacks=[earlystop, beststop],\n","        )\n","        if history.history[\"val_acc\"][-1] > 0.885:\n","            save_name = (\n","                \"/content/drive/MyDrive/Sentiment Analysis/Model/CNN_IMDB/\"\n","                + str(history.history[\"val_loss\"][-1])[2:6]\n","                + \"_\"\n","                + str(history.history[\"val_acc\"][-1])[2:6]\n","                + \".hdf5\"\n","            )\n","            num -= 1\n","            model.save(save_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"XgZP84VHne53","executionInfo":{"status":"ok","timestamp":1618980873346,"user_tz":-480,"elapsed":546777,"user":{"displayName":"柳箜铭","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64","userId":"16745569133448555194"}},"outputId":"1bed090b-0a25-4302-f2d7-57486deba735"},"source":["TrainModel(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.5983 - acc: 0.6503 - val_loss: 0.2996 - val_acc: 0.8714\n","Epoch 2/20\n","250/250 [==============================] - 12s 46ms/step - loss: 0.2246 - acc: 0.9120 - val_loss: 0.2790 - val_acc: 0.8830\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 48ms/step - loss: 0.6072 - acc: 0.6573 - val_loss: 0.3064 - val_acc: 0.8692\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2338 - acc: 0.9092 - val_loss: 0.2749 - val_acc: 0.8855\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6079 - acc: 0.6374 - val_loss: 0.2980 - val_acc: 0.8758\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2221 - acc: 0.9145 - val_loss: 0.2863 - val_acc: 0.8807\n","Epoch 3/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.0951 - acc: 0.9715 - val_loss: 0.3051 - val_acc: 0.8818\n","Epoch 1/20\n","250/250 [==============================] - 12s 46ms/step - loss: 0.6130 - acc: 0.6314 - val_loss: 0.3058 - val_acc: 0.8714\n","Epoch 2/20\n","250/250 [==============================] - 11s 45ms/step - loss: 0.2300 - acc: 0.9126 - val_loss: 0.2802 - val_acc: 0.8827\n","Epoch 3/20\n","250/250 [==============================] - 11s 44ms/step - loss: 0.0975 - acc: 0.9705 - val_loss: 0.3138 - val_acc: 0.8795\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.5996 - acc: 0.6534 - val_loss: 0.2907 - val_acc: 0.8792\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2245 - acc: 0.9146 - val_loss: 0.2803 - val_acc: 0.8813\n","Epoch 3/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.0917 - acc: 0.9728 - val_loss: 0.3099 - val_acc: 0.8830\n","Epoch 1/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.6162 - acc: 0.6301 - val_loss: 0.3054 - val_acc: 0.8688\n","Epoch 2/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.2318 - acc: 0.9071 - val_loss: 0.2740 - val_acc: 0.8868\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6030 - acc: 0.6495 - val_loss: 0.3075 - val_acc: 0.8691\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2270 - acc: 0.9115 - val_loss: 0.2723 - val_acc: 0.8868\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 50ms/step - loss: 0.6168 - acc: 0.6184 - val_loss: 0.3131 - val_acc: 0.8694\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2414 - acc: 0.9044 - val_loss: 0.2745 - val_acc: 0.8854\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 51ms/step - loss: 0.6073 - acc: 0.6382 - val_loss: 0.3081 - val_acc: 0.8686\n","Epoch 2/20\n","250/250 [==============================] - 12s 50ms/step - loss: 0.2300 - acc: 0.9079 - val_loss: 0.2688 - val_acc: 0.8870\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 51ms/step - loss: 0.6014 - acc: 0.6444 - val_loss: 0.2904 - val_acc: 0.8776\n","Epoch 2/20\n","250/250 [==============================] - 13s 51ms/step - loss: 0.2247 - acc: 0.9121 - val_loss: 0.2731 - val_acc: 0.8859\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 50ms/step - loss: 0.6064 - acc: 0.6458 - val_loss: 0.3059 - val_acc: 0.8710\n","Epoch 2/20\n","250/250 [==============================] - 12s 49ms/step - loss: 0.2313 - acc: 0.9085 - val_loss: 0.2673 - val_acc: 0.8888\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6062 - acc: 0.6475 - val_loss: 0.3319 - val_acc: 0.8534\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2330 - acc: 0.9087 - val_loss: 0.2701 - val_acc: 0.8867\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 50ms/step - loss: 0.5994 - acc: 0.6536 - val_loss: 0.3294 - val_acc: 0.8554\n","Epoch 2/20\n","250/250 [==============================] - 12s 49ms/step - loss: 0.2249 - acc: 0.9121 - val_loss: 0.2854 - val_acc: 0.8809\n","Epoch 3/20\n","250/250 [==============================] - 12s 49ms/step - loss: 0.0919 - acc: 0.9726 - val_loss: 0.3164 - val_acc: 0.8812\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6132 - acc: 0.6284 - val_loss: 0.3133 - val_acc: 0.8666\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2423 - acc: 0.9074 - val_loss: 0.2758 - val_acc: 0.8844\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6024 - acc: 0.6520 - val_loss: 0.3096 - val_acc: 0.8676\n","Epoch 2/20\n","250/250 [==============================] - 12s 49ms/step - loss: 0.2263 - acc: 0.9140 - val_loss: 0.2763 - val_acc: 0.8850\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 48ms/step - loss: 0.6110 - acc: 0.6521 - val_loss: 0.3222 - val_acc: 0.8612\n","Epoch 2/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.2317 - acc: 0.9077 - val_loss: 0.3091 - val_acc: 0.8680\n","Epoch 3/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.1035 - acc: 0.9677 - val_loss: 0.3253 - val_acc: 0.8752\n","Epoch 1/20\n","250/250 [==============================] - 13s 49ms/step - loss: 0.6006 - acc: 0.6529 - val_loss: 0.2975 - val_acc: 0.8748\n","Epoch 2/20\n","250/250 [==============================] - 12s 48ms/step - loss: 0.2316 - acc: 0.9070 - val_loss: 0.2677 - val_acc: 0.8888\n","\n","Reached 0.27 loss so cancelling training!\n","Epoch 1/20\n","250/250 [==============================] - 13s 48ms/step - loss: 0.6082 - acc: 0.6510 - val_loss: 0.3070 - val_acc: 0.8692\n","Epoch 2/20\n","250/250 [==============================] - 12s 47ms/step - loss: 0.2380 - acc: 0.9080 - val_loss: 0.2811 - val_acc: 0.8822\n","Epoch 3/20\n","250/250 [==============================] - 11s 46ms/step - loss: 0.1045 - acc: 0.9661 - val_loss: 0.3127 - val_acc: 0.8795\n","Epoch 1/20\n","250/250 [==============================] - 12s 46ms/step - loss: 0.6015 - acc: 0.6420 - val_loss: 0.3117 - val_acc: 0.8665\n","Epoch 2/20\n","250/250 [==============================] - 11s 46ms/step - loss: 0.2244 - acc: 0.9111 - val_loss: 0.2687 - val_acc: 0.8870\n","\n","Reached 0.27 loss so cancelling training!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DrRb6_yVne6K"},"source":["# del model\n","model = get_cnn_rand(max_len)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdN3TkmwnkoU"},"source":["history = model.fit(\n","            x=train_x,\n","            y=train_y,\n","            batch_size=100,\n","            epochs=20,\n","            validation_data=(test_x, test_y),\n","            # callbacks=[earlystop],\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPnpaWf_ne6K"},"source":["from tensorflow.keras.models import load_model\n","\n","# 保存模型\n","# model.save(\"10in_32unit_temp_res.h5\")\n","# del model  # deletes the existing model\n","# 导入已经训练好的模型\n","# model = load_model(\"my_model.h5\")\n","## 保存训练好的Tokenizer，和导入\n","import pickle\n","\n","# saving\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/tokenizer/200in_IMDB.pickle\", \"wb\") as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCKauqOfne6L"},"source":["loss, accuracy = model.evaluate(x=[test_x, test_x], y=test_y, verbose=0)\n","print(loss)\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkkdvFiAne6M"},"source":["### google"]},{"cell_type":"code","metadata":{"id":"dk2R4i81ne6M"},"source":["def load_google():\n","    Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(\n","        r\"E:\\CS\\MLT\\GoogleNews-vectors-negative300.bin\", binary=True\n","    )\n","\n","    vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]  # 存储 所有的 词语\n","\n","    # word_index = {\" \": 0}  # 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n","    embeddings_index = {}  # 初始化`[word : vector]`字典\n","\n","    for i in range(len(vocab_list)):\n","        # print(i)\n","        word = vocab_list[i]  # 每个词语\n","        # word_index[word] = i + 1 # 词语：序号\n","        embeddings_index[word] = Word2VecModel.wv[word]  # 词语：词向量\n","        # embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵\n","    return embeddings_index\n","\n","\n","def load_fasttext_embeddings():\n","    glove_dir = r\"E:\\CS\\MLT\\glove.6B\"\n","    embeddings_index = {}\n","    f = open(os.path.join(glove_dir, \"glove.6B.50d.txt\"), encoding=\"utf-8\")\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype=\"float32\")\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print(\"Found %s word vectors.\" % len(embeddings_index))\n","    return embeddings_index\n","\n","\n","def create_embeddings_matrix(embeddings_index, vocabulary, embedding_dim=100):\n","    embeddings_matrix = np.random.rand(len(vocabulary) + 1, embedding_dim)\n","    for i, word in enumerate(vocabulary):\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embeddings_matrix[i] = embedding_vector\n","    print(\"Matrix shape: {}\".format(embeddings_matrix.shape))\n","    return embeddings_matrix\n","\n","\n","embeddings_index = load_google()\n","embeddings_matrix = create_embeddings_matrix(embeddings_index, word_index, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANOlCJ1tne6P"},"source":["import gensim\n","\n","Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(\n","    r\"E:\\CS\\MLT\\GoogleNews-vectors-negative300.bin\", binary=True\n",")\n","\n","vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]  # 存储 所有的 词语\n","\n","word_index = {\" \": 0}  # 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n","word_vector = {}  # 初始化`[word : vector]`字典\n","\n","for i in range(len(vocab_list)):\n","    # print(i)\n","    word = vocab_list[i]  # 每个词语\n","    #     word_index[word] = i + 1 # 词语：序号\n","    #     word_vector[word] = Word2VecModel.wv[word] # 词语：词向量\n","    embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵"],"execution_count":null,"outputs":[]}]}