{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ExtractMetrics4SST.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "mount_file_id": "1TfRZGoMflXL7MtQVAzD1sKYPDjRsPpB3",
   "authorship_tag": "ABX9TyMCjN5VV/54nuOB1Z1sJh4p"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeuSKlYC_1yr"
   },
   "source": [
    "# CoreNLP server"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bioNhcRu-qa6"
   },
   "source": [
    "# init and start CoreNLP server\n",
    "import os\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d1tOuFkKMAv",
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1618819738784,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     },
     "user_tz": -480
    },
    "outputId": "05303bbd-6e3a-4ab2-e601-5b6418dcb848"
   },
   "source": [
    "# !wget \"https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\"\n",
    "# !unzip \"stanford-corenlp-latest.zip\""
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZK29NaSLqvr",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1618819914219,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     },
     "user_tz": -480
    },
    "outputId": "f29afe85-6d70-48cd-d1ce-e5d3de0af21e"
   },
   "source": [
    "cd ./stanford-corenlp-4.5.1/\n",
    "!nohup java -mx5g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 10000 &\n",
    "\n",
    "# shut down CoreNLP server\n",
    "\n",
    "# !ps aux | grep java\n",
    "# !kill 719"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-84431c963d7b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-8-84431c963d7b>\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    cd ./stanford-corenlp-4.5.1/\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCrb5qdmM7MP",
    "executionInfo": {
     "elapsed": 2940,
     "status": "ok",
     "timestamp": 1618832175324,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     },
     "user_tz": -480
    },
    "outputId": "ddeced36-f7b6-4cfa-9ae4-7ee362568e51"
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw-1.4')"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xuzhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xuzhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\xuzhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\xuzhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\xuzhi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olEdIHI7ADDM"
   },
   "source": [
    "# Code of extracting metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBsGzFlR9nTk"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.parse.corenlp import CoreNLPServer, CoreNLPParser\n",
    "\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "parser = CoreNLPParser(url='http://localhost:9001')"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB3mAjVi8k9-"
   },
   "source": [
    "## Input feature metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Bar_g3v_O2Y"
   },
   "source": [
    "def lexical(sentence):\n",
    "    doc = nlp_pipeline(sentence)\n",
    "    oov = 0\n",
    "    # verb\n",
    "    VB = 0\n",
    "    # noun\n",
    "    NN = 0\n",
    "    # adj\n",
    "    JJ = 0\n",
    "    # adv\n",
    "    RB = 0\n",
    "    # conj\n",
    "    CONJ = 0\n",
    "\n",
    "    length = 0\n",
    "\n",
    "    for token in doc:\n",
    "        length += 1\n",
    "        if token.tag_.startswith(\"J\"):\n",
    "            JJ += 1\n",
    "        if token.tag_.startswith(\"N\"):\n",
    "            NN += 1\n",
    "        if token.tag_.startswith(\"R\"):\n",
    "            RB += 1\n",
    "        if token.tag_.startswith(\"V\"):\n",
    "            VB += 1\n",
    "        if token.tag_.startswith(\"C\"):\n",
    "            CONJ += 1\n",
    "\n",
    "    return VB, NN, JJ, RB, CONJ, len(list(doc.sents)), length\n",
    "\n",
    "# Degree of polysemy\n",
    "def polysemy(clean_line):\n",
    "    polysemyCount = 0\n",
    "    words = clean_line.split(\" \")\n",
    "    for w in words:\n",
    "        polysemyCount += len(wn.synsets(w))\n",
    "    return float(polysemyCount) / float(len(words))\n",
    "\n",
    "\n",
    "# dependency distance\n",
    "def depend_dist(sentence):\n",
    "    doc = nlp_pipeline(sentence)\n",
    "    sum_dist = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_dist = 0\n",
    "        for token in sent:\n",
    "            if not token.is_punct:\n",
    "                for child in token.children:\n",
    "                    sent_dist += abs(token.i - child.i)\n",
    "        sum_dist += sent_dist\n",
    "    return float(sum_dist) / float(len(list(doc.sents)))\n",
    "\n",
    "\n",
    "# height of constituency parsing tree\n",
    "def const_parse(sentences):\n",
    "    doc = nlp_pipeline(sentences)\n",
    "    sum_height = 0\n",
    "    non_term_count = 0\n",
    "    term_count = 0\n",
    "    for sent in doc.sents:\n",
    "        if sent:\n",
    "            try:\n",
    "                res = next(parser.raw_parse(sent.text))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(sent)\n",
    "            else:\n",
    "                sum_height += res.height()\n",
    "                non_term_count += len(list(res.leaves()))\n",
    "                term_count += len(list(res.subtrees()))\n",
    "\n",
    "    return (\n",
    "        float(sum_height) / float(len(list(doc.sents))),\n",
    "        float(non_term_count) / float(term_count),\n",
    "    )\n",
    "\n",
    "\n",
    "# process sentiment score\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentiment(word, tag):\n",
    "    \"\"\" \n",
    "    returns list of pos neg and objective score. But returns empty list \n",
    "    if not present in senti wordnet. \n",
    "    \"\"\"\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return [0, 1, 0]\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [\n",
    "            swn_synset.pos_score(), \n",
    "            swn_synset.neg_score(), \n",
    "            swn_synset.obj_score(),\n",
    "    ]\n",
    "\n",
    "\n",
    "def senti_pro(clean_sent):\n",
    "    flip_count = 0\n",
    "    words = clean_sent.split(\" \")\n",
    "    pos_val = nltk.pos_tag(words)\n",
    "    senti_ret = np.array([get_sentiment(x, y) for (x, y) in pos_val])\n",
    "    senti_val = senti_ret.T[0] - senti_ret.T[1]\n",
    "    senti_score = abs(senti_ret.T[0].sum() - senti_ret.T[1].sum())\n",
    "    \n",
    "    # sentiment flip count\n",
    "    for i in range(senti_val.shape[0] - 1):\n",
    "        if senti_val[i] * senti_val[i + 1] < 0:\n",
    "            flip_count += 1\n",
    "\n",
    "    return senti_score, flip_count"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE9CjLqKCqBR"
   },
   "source": [
    "## Fetch&Process data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vdDiSldbFDLC"
   },
   "source": [
    "def modi_data(obj_df):\n",
    "    obj_df = obj_df[obj_df[\"label\"] != 2]\n",
    "    # obj_df[\"label\"] = obj_df[\"label\"].astype(string)\n",
    "    obj_df.label = obj_df.label.replace(0, \"neg\")\n",
    "    obj_df.label = obj_df.label.replace(1, \"neg\")\n",
    "    obj_df.label = obj_df.label.replace(3, \"pos\")\n",
    "    obj_df.label = obj_df.label.replace(4, \"pos\")\n",
    "    obj_df = obj_df.fillna(0)\n",
    "    # obj_df[\"label\"] = obj_df[\"label\"].astype(int)\n",
    "    return obj_df\n",
    "\n",
    "\n",
    "def clean_sent(sentence):\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pre_proc(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    if len(words) > max_len:\n",
    "        sentence = \" \".join(words[:max_len])\n",
    "    return sentence"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "83IzbZBr-ND9"
   },
   "source": [
    "# set global var\n",
    "\n",
    "data_path = \"./SST-2/\"\n",
    "tokenizer_path = \"./tokenizer/49in_SST.pickle\"\n",
    "max_len = 49\n",
    "remove_threshold = 1e-5"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEK6WSlyl0j0",
    "executionInfo": {
     "elapsed": 4604,
     "status": "ok",
     "timestamp": 1618842635631,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     },
     "user_tz": -480
    },
    "outputId": "e6f3e207-255c-4a25-d703-7409ea2826ac"
   },
   "source": [
    "train_df = pd.read_csv(\n",
    "    data_path + \"train.csv\", header=None, sep=\"\\t\", names=[\"label\", \"text\"]\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    data_path + \"test.csv\", header=None, sep=\"\\t\", names=[\"label\", \"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "with open(tokenizer_path, \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "test_df = modi_data(test_df)\n",
    "test_sentence = test_df.text.tolist()\n",
    "# test_clean_tokens = [clean_sent(i) for i in test_sentence]\n",
    "test_x = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_x = pad_sequences(test_x, maxlen=max_len, padding=\"post\")\n",
    "test_label = test_df.label.tolist()\n",
    "test_sentence = list(map(pre_proc, test_sentence))\n",
    "\n",
    "test_y = np.array(test_label)\n",
    "test_y[test_y == 'pos'] = 1\n",
    "test_y[test_y == 'neg'] = 0\n",
    "test_y = test_y.reshape(-1,1).astype(int)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\anaconda\\envs\\tf1.14.0\\lib\\site-packages\\pandas\\core\\generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YGA8aCQl6YtI"
   },
   "source": [
    "# clean_line = [\" \".join(n) for n in test_clean_tokens]\n",
    "inp_feature_df = pd.DataFrame(\n",
    "    {\"Sentence\": test_sentence, \n",
    "    #  \"CleanLine\": clean_line, \n",
    "     \"TrueRes\": test_y.reshape(-1)}\n",
    ")\n",
    "\n",
    "inp_feature_df[\"VBCount\"], inp_feature_df[\"NNCount\"], \\\n",
    "inp_feature_df[\"JJCount\"], inp_feature_df[\"RBCount\"], \\\n",
    "inp_feature_df[\"ConjCount\"], inp_feature_df[\"SentCount\"], \\\n",
    "inp_feature_df[\"Length\"] = zip(\n",
    "    *inp_feature_df[\"Sentence\"].map(lexical)\n",
    ")\n",
    "\n",
    "inp_feature_df[\"Polysemy\"] = inp_feature_df[\"Sentence\"].map(polysemy)\n",
    "\n",
    "# dependency distance\n",
    "inp_feature_df[\"DependDist\"] = inp_feature_df[\"Sentence\"].map(depend_dist)\n",
    "\n",
    "# sentiment process\n",
    "inp_feature_df[\"SentiScore\"], inp_feature_df[\"SentiFlip\"] = zip(\n",
    "    *inp_feature_df[\"Sentence\"].map(senti_pro)\n",
    ")\n",
    "\n",
    "# constituency parsing tree\n",
    "inp_feature_df[\"ConstHeight\"], inp_feature_df[\"TerminalRatio\"] = \\\n",
    "zip(*inp_feature_df[\"Sentence\"].map(const_parse))"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vmmAlZJhFJkO"
   },
   "source": [
    "inp_feature_df.to_csv(\"./Metrics/SST_inp.csv\", index=False)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EYjMh3-G0Wp"
   },
   "source": [
    "## Output feature metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fpcnoH2h8y44"
   },
   "source": [
    "def get_last_layer_model(model):\n",
    "    layer_names = [layer.name for layer in model.layers]\n",
    "    layer_output = model.get_layer(layer_names[-2]).output\n",
    "    ret = Model(model.input, layer_output)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_train_at(train_df, last_layer_model):\n",
    "    train_df = modi_data(train_df)\n",
    "    pos_df = train_df[train_df[\"label\"] == \"pos\"]\n",
    "    neg_df = train_df[train_df[\"label\"] == \"neg\"]\n",
    "\n",
    "    pos_sentence = pos_df.text.tolist()\n",
    "    pos_x = tokenizer.texts_to_sequences(pos_sentence)\n",
    "    pos_x = pad_sequences(pos_x, maxlen = max_len, padding=\"post\")\n",
    "\n",
    "    neg_sentence = neg_df.text.tolist()\n",
    "    neg_x = tokenizer.texts_to_sequences(neg_sentence)\n",
    "    neg_x = pad_sequences(neg_x, maxlen = max_len, padding=\"post\")\n",
    "    \n",
    "    ret = {}\n",
    "    ret[\"pos\"] = last_layer_model.predict(pos_x)\n",
    "    ret[\"neg\"] = last_layer_model.predict(neg_x)\n",
    "    return ret\n",
    "\n",
    "def get_kernels(train_at):\n",
    "    removed_cols={'pos': [], 'neg': []}\n",
    "\n",
    "    for i in range(train_at[\"pos\"].T.shape[0]):\n",
    "        if np.var(train_at[\"pos\"].T[i]) < remove_threshold:\n",
    "            removed_cols['pos'].append(i)\n",
    "    for i in range(train_at[\"neg\"].T.shape[0]):\n",
    "        if np.var(train_at[\"neg\"].T[i]) < remove_threshold:\n",
    "            removed_cols['neg'].append(i)\n",
    "\n",
    "    pos_vals = np.delete(train_at[\"pos\"].T, removed_cols['pos'], axis=0)\n",
    "    neg_vals = np.delete(train_at[\"neg\"].T, removed_cols['neg'], axis=0)\n",
    "\n",
    "    kernels={}\n",
    "    kernels[\"pos\"] = stats.gaussian_kde(pos_vals)\n",
    "    kernels[\"neg\"] = stats.gaussian_kde(neg_vals)\n",
    "\n",
    "    return kernels, removed_cols\n",
    "    \n",
    "\n",
    "def get_lsa(kernels, removed_cols, test_pred, test_label):\n",
    "    lsa=[]\n",
    "    \n",
    "    for i in range(len(test_pred)):\n",
    "        value = np.delete(test_pred[i], removed_cols[test_label[i]])\n",
    "        temp = np.negative(np.log(kernels[test_label[i]](value)))\n",
    "        \n",
    "        lsa.append(temp[0])\n",
    "    \n",
    "    return lsa\n",
    "\n",
    "def find_closest_at(at, train_at):\n",
    "    \"\"\"The closest distance between subject AT and training ATs.\n",
    "    Args:\n",
    "        at (list): List of activation traces of an input.        \n",
    "        train_at (list): List of activation traces in training set (filtered)\n",
    "        \n",
    "    Returns:\n",
    "        dist (int): The closest distance.\n",
    "        at (list): Training activation trace that has the closest distance.\n",
    "    \"\"\"\n",
    "\n",
    "    dist = np.linalg.norm(at - train_at, axis=1)\n",
    "    return (min(dist), train_at[np.argmin(dist)])\n",
    "\n",
    "\n",
    "def get_dsa(test_pred, test_label, train_at):\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(len(test_pred)):\n",
    "        label = test_label[i]\n",
    "        at = test_pred[i]\n",
    "        a_dist, a_dot = find_closest_at(at, train_at[label])\n",
    "        b_dist, _ = find_closest_at(\n",
    "            a_dot, train_at[list(set([\"pos\", \"neg\"]) - set([label]))[0]]\n",
    "        )\n",
    "        ret.append(a_dist / b_dist)\n",
    "    return ret\n",
    "\n",
    "def val_to_res(val):\n",
    "    if val > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def deep_gini(val):\n",
    "    ret = float(1 - pow(val, 2) - pow(1 - val, 2))\n",
    "    return ret\n"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wnRpXLYumtNH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618849943462,
     "user_tz": -480,
     "elapsed": 1801,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    }
   },
   "source": [
    "def Process(model_path, out_path):\n",
    "    inp_feature_df = pd.read_csv(\n",
    "        \"./Metrics/SST_inp.csv\",\n",
    "    )\n",
    "\n",
    "    dirs = os.listdir(model_path)\n",
    "    for i in dirs:\n",
    "        if os.path.splitext(i)[1] == \".hdf5\":\n",
    "            model = get_Trans(max_len)\n",
    "            model.load_weights(model_path + i)\n",
    "            # model = load_model(model_path + i)\n",
    "\n",
    "            # verify\n",
    "            test_scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "            print(i, test_scores)\n",
    "            if test_scores[1] < float(\"0.\" + i.split('_')[1][:4]):\n",
    "                raise Exception(\"Invalid acc! \" + i, test_scores[1])\n",
    "            \n",
    "            pred_val = model.predict(test_x)\n",
    "            pred_res = [val_to_res(n) for n in pred_val]\n",
    "            output_feature_df = pd.DataFrame(\n",
    "                {\"PredVal\": pred_val[:, 0], \"PredRes\": pred_res}\n",
    "            )\n",
    "            output_feature_df[\"DeepGini\"] = \\\n",
    "            output_feature_df[\"PredVal\"].map(deep_gini)\n",
    "            \n",
    "            last_layer_model = get_last_layer_model(model)\n",
    "            test_pred = last_layer_model.predict(test_x)\n",
    "            train_at = get_train_at(train_df, last_layer_model)\n",
    "            kernels, removed_cols = get_kernels(train_at)\n",
    "            lsa = get_lsa(kernels, removed_cols, test_pred, test_label)\n",
    "            dsa = get_dsa(test_pred, test_label, train_at)\n",
    "            output_feature_df[\"LSA\"] = lsa\n",
    "            output_feature_df[\"DSA\"] = dsa\n",
    "            \n",
    "            res_df = pd.concat([inp_feature_df, output_feature_df], axis=1)\n",
    "\n",
    "            res_df[\"isRight\"] = res_df.apply(\n",
    "                lambda x: x[\"TrueRes\"] == x[\"PredRes\"], axis=1\n",
    "            )\n",
    "            res_df.to_csv(out_path + os.path.splitext(i)[0] + \".csv\")\n",
    "            print(\"model \" + os.path.splitext(i)[0] + \" is saved\")\n",
    "    print(\"done!\")"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0porwog0hPQ6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618850024908,
     "user_tz": -480,
     "elapsed": 82763,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    },
    "outputId": "d26d311e-819c-48ae-c581-7408bf7f2a25"
   },
   "source": [
    "model_path = \"./Model/Trans_SST/\"\n",
    "out_path = \"./Metrics/Trans_SST/\"\n",
    "\n",
    "Process(model_path, out_path)"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4484_8132.hdf5 [0.44842463731765747, 0.8127402663230896]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "('Invalid acc! 4484_8132.hdf5', 0.8127402663230896)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-32-a34071b675cb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mout_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"./Metrics/Trans_SST/\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mProcess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-31-0ce2b951cc1c>\u001B[0m in \u001B[0;36mProcess\u001B[1;34m(model_path, out_path)\u001B[0m\n\u001B[0;32m     15\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_scores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtest_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"0.\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'_'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Invalid acc! \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m             \u001B[0mpred_val\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_x\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mException\u001B[0m: ('Invalid acc! 4484_8132.hdf5', 0.8127402663230896)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gnl1iQ8fXVi9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618849930786,
     "user_tz": -480,
     "elapsed": 1954,
     "user": {
      "displayName": "柳箜铭",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwCICr9ID5QfKQ8gBpGLnwTSXha1wSAt2FGshyhQ=s64",
      "userId": "16745569133448555194"
     }
    }
   },
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Add,\n",
    "    Lambda,\n",
    "    Dropout,\n",
    "    concatenate,\n",
    ")\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'maxlen': self.maxlen,\n",
    "            'embed_dim': self.embed_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def get_Trans(max_len):\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_len, len(tokenizer.word_index) + 1, embed_dim)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\", name=\"input\")\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    # x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    o = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=o)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "    return model"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
